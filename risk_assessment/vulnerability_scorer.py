#!/usr/bin/env python3
"""
Vulnerability Scorer
Comprehensive AI security vulnerability scoring and risk assessment framework.
"""

import json
import logging
import math
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Union, Any
from dataclasses import dataclass, asdict
from enum import Enum
import numpy as np

class CriticalityLevel(Enum):
    """System criticality levels."""
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4

class ExposureLevel(Enum):
    """System exposure levels."""
    INTERNAL = 1
    PARTNER = 2
    CUSTOMER_FACING = 3
    PUBLIC = 4

class VulnerabilityType(Enum):
    """Types of AI vulnerabilities."""
    ADVERSARIAL_ATTACKS = "adversarial_attacks"
    DATA_POISONING = "data_poisoning"
    MODEL_INVERSION = "model_inversion"
    MEMBERSHIP_INFERENCE = "membership_inference"
    PROMPT_INJECTION = "prompt_injection"
    JAILBREAKING = "jailbreaking"
    BACKDOOR_ATTACKS = "backdoor_attacks"
    EVASION_ATTACKS = "evasion_attacks"
    INFORMATION_LEAKAGE = "information_leakage"
    BIAS_EXPLOITATION = "bias_exploitation"

@dataclass
class VulnerabilityFinding:
    """Represents a single vulnerability finding."""
    id: str
    name: str
    vulnerability_type: VulnerabilityType
    severity_score: float  # 0-10 scale
    exploitability_score: float  # 0-10 scale
    impact_score: float  # 0-10 scale
    confidence_level: float  # 0-1 scale
    description: str
    affected_systems: List[str]
    attack_vectors: List[str]
    evidence: Dict[str, Any]
    mitigation_recommendations: List[str]
    cvss_vector: Optional[str] = None
    
    @property
    def base_score(self) -> float:
        """Calculate CVSS-like base score."""
        return min(10.0, (self.exploitability_score + self.impact_score) / 2)
    
    @property
    def environmental_score(self) -> float:
        """Calculate environmental score based on system criticality."""
        return self.base_score * self.confidence_level

@dataclass
class SystemProfile:
    """Profile of an AI system for risk assessment."""
    name: str
    system_type: str
    criticality: CriticalityLevel
    exposure: ExposureLevel
    data_sensitivity: str  # "public", "internal", "confidential", "restricted"
    compliance_requirements: List[str]
    security_controls: List[str]
    business_impact: Dict[str, float]
    technical_details: Dict[str, Any]

@dataclass
class ThreatActor:
    """Represents a threat actor profile."""
    name: str
    sophistication_level: int  # 1-5 scale
    motivation: str
    capabilities: List[str]
    attack_methods: List[str]
    likelihood_score: float  # 0-1 scale

class VulnerabilityScorer:
    """Main vulnerability scoring and risk assessment engine."""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.vulnerability_weights = {
            VulnerabilityType.ADVERSARIAL_ATTACKS: 0.8,
            VulnerabilityType.DATA_POISONING: 0.9,
            VulnerabilityType.MODEL_INVERSION: 0.7,
            VulnerabilityType.MEMBERSHIP_INFERENCE: 0.6,
            VulnerabilityType.PROMPT_INJECTION: 0.8,
            VulnerabilityType.JAILBREAKING: 0.7,
            VulnerabilityType.BACKDOOR_ATTACKS: 0.9,
            VulnerabilityType.EVASION_ATTACKS: 0.6,
            VulnerabilityType.INFORMATION_LEAKAGE: 0.8,
            VulnerabilityType.BIAS_EXPLOITATION: 0.5
        }
        self.industry_benchmarks = self._load_industry_benchmarks()
        self.compliance_frameworks = self._load_compliance_frameworks()
    
    def _load_industry_benchmarks(self) -> Dict[str, Dict]:
        """Load industry-specific security benchmarks."""
        return {
            "financial_services": {
                "max_acceptable_risk": 3.0,
                "critical_vulnerabilities_threshold": 0,
                "high_vulnerabilities_threshold": 2,
                "compliance_requirements": ["pci_dss", "sox", "gdpr", "basel_iii"]
            },
            "healthcare": {
                "max_acceptable_risk": 2.5,
                "critical_vulnerabilities_threshold": 0,
                "high_vulnerabilities_threshold": 1,
                "compliance_requirements": ["hipaa", "hitech", "gdpr"]
            },
            "technology": {
                "max_acceptable_risk": 4.0,
                "critical_vulnerabilities_threshold": 1,
                "high_vulnerabilities_threshold": 5,
                "compliance_requirements": ["gdpr", "ccpa", "iso_27001"]
            },
            "government": {
                "max_acceptable_risk": 2.0,
                "critical_vulnerabilities_threshold": 0,
                "high_vulnerabilities_threshold": 1,
                "compliance_requirements": ["fisma", "nist_csf", "fedramp"]
            }
        }
    
    def _load_compliance_frameworks(self) -> Dict[str, Dict]:
        """Load compliance framework requirements."""
        return {
            "eu_ai_act": {
                "high_risk_categories": ["biometric", "critical_infrastructure", "education", "employment"],
                "requirements": ["risk_assessment", "data_governance", "transparency", "human_oversight"],
                "penalty_multiplier": 2.0
            },
            "gdpr": {
                "data_protection_requirements": ["consent", "data_minimization", "purpose_limitation"],
                "penalty_multiplier": 1.5
            },
            "pci_dss": {
                "security_requirements": ["encryption", "access_control", "monitoring", "testing"],
                "penalty_multiplier": 1.3
            },
            "hipaa": {
                "privacy_requirements": ["minimum_necessary", "access_control", "audit_logs"],
                "penalty_multiplier": 1.8
            }
        }
    
    def score_vulnerability(
        self,
        vulnerability_data: Dict[str, Any],
        system_profile: SystemProfile,
        threat_context: Optional[Dict[str, Any]] = None
    ) -> VulnerabilityFinding:
        """
        Score a single vulnerability finding.
        
        Args:
            vulnerability_data: Raw vulnerability data from security tests
            system_profile: Profile of the affected system
            threat_context: Additional threat intelligence context
            
        Returns:
            VulnerabilityFinding with calculated scores
        """
        vuln_type = VulnerabilityType(vulnerability_data.get('type', 'adversarial_attacks'))
        
        # Calculate base scores
        exploitability = self._calculate_exploitability_score(vulnerability_data, system_profile)
        impact = self._calculate_impact_score(vulnerability_data, system_profile)
        
        # Apply vulnerability type weight
        type_weight = self.vulnerability_weights.get(vuln_type, 0.7)
        severity = (exploitability + impact) / 2 * type_weight
        
        # Calculate confidence based on evidence quality
        confidence = self._calculate_confidence_score(vulnerability_data)
        
        vulnerability = VulnerabilityFinding(
            id=vulnerability_data.get('id', f"vuln_{datetime.now().strftime('%Y%m%d_%H%M%S')}"),
            name=vulnerability_data.get('name', f"AI Vulnerability - {vuln_type.value}"),
            vulnerability_type=vuln_type,
            severity_score=min(10.0, severity),
            exploitability_score=exploitability,
            impact_score=impact,
            confidence_level=confidence,
            description=vulnerability_data.get('description', ''),
            affected_systems=[system_profile.name],
            attack_vectors=vulnerability_data.get('attack_vectors', []),
            evidence=vulnerability_data.get('evidence', {}),
            mitigation_recommendations=vulnerability_data.get('mitigations', []),
            cvss_vector=self._generate_cvss_vector(exploitability, impact, vuln_type)
        )
        
        self.logger.info(f"Scored vulnerability {vulnerability.name}: "
                        f"Severity={severity:.1f}, Exploitability={exploitability:.1f}, "
                        f"Impact={impact:.1f}, Confidence={confidence:.2f}")
        
        return vulnerability
    
    def _calculate_exploitability_score(
        self, 
        vuln_data: Dict[str, Any], 
        system_profile: SystemProfile
    ) -> float:
        """Calculate exploitability score (0-10)."""
        base_exploitability = vuln_data.get('exploitability', 5.0)
        
        # Adjust for system exposure
        exposure_multipliers = {
            ExposureLevel.INTERNAL: 0.7,
            ExposureLevel.PARTNER: 0.8,
            ExposureLevel.CUSTOMER_FACING: 0.9,
            ExposureLevel.PUBLIC: 1.0
        }
        exposure_factor = exposure_multipliers.get(system_profile.exposure, 1.0)
        
        # Adjust for security controls
        security_controls_factor = max(0.3, 1.0 - len(system_profile.security_controls) * 0.1)
        
        # Adjust for attack complexity
        complexity_factor = 1.0
        if 'attack_complexity' in vuln_data:
            complexity_map = {'low': 1.0, 'medium': 0.8, 'high': 0.6}
            complexity_factor = complexity_map.get(vuln_data['attack_complexity'], 0.8)
        
        exploitability = base_exploitability * exposure_factor * security_controls_factor * complexity_factor
        return min(10.0, max(0.0, exploitability))
    
    def _calculate_impact_score(
        self, 
        vuln_data: Dict[str, Any], 
        system_profile: SystemProfile
    ) -> float:
        """Calculate impact score (0-10)."""
        base_impact = vuln_data.get('impact', 5.0)
        
        # Adjust for system criticality
        criticality_multipliers = {
            CriticalityLevel.LOW: 0.5,
            CriticalityLevel.MEDIUM: 0.7,
            CriticalityLevel.HIGH: 0.9,
            CriticalityLevel.CRITICAL: 1.0
        }
        criticality_factor = criticality_multipliers.get(system_profile.criticality, 0.7)
        
        # Adjust for data sensitivity
        sensitivity_multipliers = {
            'public': 0.4,
            'internal': 0.6,
            'confidential': 0.8,
            'restricted': 1.0
        }
        sensitivity_factor = sensitivity_multipliers.get(system_profile.data_sensitivity, 0.6)
        
        # Business impact factor
        business_impact_factor = system_profile.business_impact.get('financial', 0.5) + \
                               system_profile.business_impact.get('operational', 0.3) + \
                               system_profile.business_impact.get('reputational', 0.2)
        
        impact = base_impact * criticality_factor * sensitivity_factor * business_impact_factor
        return min(10.0, max(0.0, impact))
    
    def _calculate_confidence_score(self, vuln_data: Dict[str, Any]) -> float:
        """Calculate confidence score based on evidence quality."""
        evidence = vuln_data.get('evidence', {})
        
        # Base confidence from test results
        base_confidence = vuln_data.get('confidence', 0.7)
        
        # Adjust for evidence quality
        evidence_factors = {
            'proof_of_concept': 0.2,
            'exploit_code': 0.3,
            'test_results': 0.2,
            'expert_validation': 0.2,
            'automated_detection': 0.1
        }
        
        evidence_score = sum(evidence_factors.get(key, 0) for key in evidence.keys())
        confidence = min(1.0, base_confidence + evidence_score)
        
        return confidence
    
    def _generate_cvss_vector(
        self, 
        exploitability: float, 
        impact: float, 
        vuln_type: VulnerabilityType
    ) -> str:
        """Generate CVSS-like vector string."""
        # Map scores to CVSS metrics
        av = "N" if exploitability > 7 else "A" if exploitability > 4 else "L"
        ac = "L" if exploitability > 7 else "M" if exploitability > 4 else "H"
        pr = "N" if exploitability > 8 else "L" if exploitability > 5 else "H"
        ui = "N" if exploitability > 6 else "R"
        
        c = "H" if impact > 7 else "L" if impact > 4 else "N"
        i = "H" if impact > 7 else "L" if impact > 4 else "N"
        a = "H" if impact > 7 else "L" if impact > 4 else "N"
        
        return f"CVSS:3.1/AV:{av}/AC:{ac}/PR:{pr}/UI:{ui}/S:U/C:{c}/I:{i}/A:{a}"
    
    def calculate_comprehensive_risk(
        self,
        assessment_config: Dict[str, Any],
        attack_test_results: Optional[Dict[str, Any]] = None,
        defense_test_results: Optional[Dict[str, Any]] = None,
        industry_benchmarks: Optional[str] = None,
        regulatory_requirements: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Calculate comprehensive risk assessment for AI systems.
        
        Args:
            assessment_config: System configuration and threat model
            attack_test_results: Results from red team testing
            defense_test_results: Results from blue team validation
            industry_benchmarks: Industry sector for benchmarking
            regulatory_requirements: Applicable regulatory frameworks
            
        Returns:
            Comprehensive risk assessment results
        """
        self.logger.info("Starting comprehensive risk assessment")
        
        systems = []
        for sys_config in assessment_config.get('ai_systems_inventory', []):
            system_profile = SystemProfile(
                name=sys_config['name'],
                system_type=sys_config.get('type', 'ai_system'),
                criticality=CriticalityLevel[sys_config.get('criticality', 'MEDIUM').upper()],
                exposure=ExposureLevel[sys_config.get('exposure', 'INTERNAL').upper()],
                data_sensitivity=sys_config.get('data_sensitivity', 'internal'),
                compliance_requirements=sys_config.get('compliance_requirements', []),
                security_controls=sys_config.get('security_controls', []),
                business_impact=sys_config.get('business_impact', {'financial': 0.5, 'operational': 0.3, 'reputational': 0.2}),
                technical_details=sys_config.get('technical_details', {})
            )
            systems.append(system_profile)
        
        # Process vulnerabilities from test results
        vulnerabilities = []
        if attack_test_results:
            for system_name, test_results in attack_test_results.items():
                system_profile = next((s for s in systems if s.name == system_name), systems[0])
                
                for vuln_data in test_results.get('vulnerabilities', []):
                    vulnerability = self.score_vulnerability(vuln_data, system_profile)
                    vulnerabilities.append(vulnerability)
        
        # Calculate system-level risk scores
        system_risk_scores = {}
        for system in systems:
            system_vulns = [v for v in vulnerabilities if system.name in v.affected_systems]
            system_risk = self._calculate_system_risk_score(system, system_vulns)
            system_risk_scores[system.name] = system_risk
        
        # Calculate overall organizational risk
        overall_risk = self._calculate_organizational_risk(systems, vulnerabilities, system_risk_scores)
        
        # Apply industry benchmarks and compliance requirements
        compliance_gaps = []
        if regulatory_requirements:
            compliance_gaps = self._assess_compliance_gaps(vulnerabilities, regulatory_requirements)
        
        benchmark_comparison = {}
        if industry_benchmarks and industry_benchmarks in self.industry_benchmarks:
            benchmark_comparison = self._compare_to_industry_benchmark(
                overall_risk, vulnerabilities, industry_benchmarks
            )
        
        results = {
            'assessment_timestamp': datetime.now().isoformat(),
            'overall_risk_score': overall_risk,
            'system_risk_scores': system_risk_scores,
            'vulnerabilities': [asdict(v) for v in vulnerabilities],
            'vulnerability_summary': self._generate_vulnerability_summary(vulnerabilities),
            'compliance_gaps': compliance_gaps,
            'industry_benchmark_comparison': benchmark_comparison,
            'risk_trend_analysis': self._analyze_risk_trends(vulnerabilities),
            'prioritized_recommendations': self._generate_prioritized_recommendations(vulnerabilities, systems)
        }
        
        self.logger.info(f"Risk assessment completed. Overall risk score: {overall_risk:.2f}")
        
        return results
    
    def _calculate_system_risk_score(
        self, 
        system: SystemProfile, 
        vulnerabilities: List[VulnerabilityFinding]
    ) -> Dict[str, float]:
        """Calculate risk score for a single system."""
        if not vulnerabilities:
            return {
                'risk_score': 0.0,
                'vulnerability_count': 0,
                'critical_vulnerabilities': 0,
                'high_vulnerabilities': 0,
                'medium_vulnerabilities': 0,
                'low_vulnerabilities': 0
            }
        
        # Weight vulnerabilities by severity and confidence
        weighted_scores = []
        severity_counts = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0}
        
        for vuln in vulnerabilities:
            weighted_score = vuln.environmental_score
            weighted_scores.append(weighted_score)
            
            # Categorize by severity
            if vuln.severity_score >= 9.0:
                severity_counts['critical'] += 1
            elif vuln.severity_score >= 7.0:
                severity_counts['high'] += 1
            elif vuln.severity_score >= 4.0:
                severity_counts['medium'] += 1
            else:
                severity_counts['low'] += 1
        
        # Calculate aggregate risk score
        if weighted_scores:
            # Use root mean square to give higher weight to severe vulnerabilities
            risk_score = math.sqrt(sum(score ** 2 for score in weighted_scores) / len(weighted_scores))
        else:
            risk_score = 0.0
        
        return {
            'risk_score': min(10.0, risk_score),
            'vulnerability_count': len(vulnerabilities),
            'critical_vulnerabilities': severity_counts['critical'],
            'high_vulnerabilities': severity_counts['high'],
            'medium_vulnerabilities': severity_counts['medium'],
            'low_vulnerabilities': severity_counts['low']
        }
    
    def _calculate_organizational_risk(
        self,
        systems: List[SystemProfile],
        vulnerabilities: List[VulnerabilityFinding],
        system_scores: Dict[str, Dict[str, float]]
    ) -> float:
        """Calculate overall organizational risk score."""
        if not system_scores:
            return 0.0
        
        # Weight system risks by criticality
        criticality_weights = {
            CriticalityLevel.CRITICAL: 1.0,
            CriticalityLevel.HIGH: 0.8,
            CriticalityLevel.MEDIUM: 0.6,
            CriticalityLevel.LOW: 0.4
        }
        
        weighted_risks = []
        for system in systems:
            system_risk = system_scores.get(system.name, {}).get('risk_score', 0.0)
            weight = criticality_weights.get(system.criticality, 0.6)
            weighted_risks.append(system_risk * weight)
        
        # Calculate organizational risk using weighted average
        if weighted_risks:
            org_risk = sum(weighted_risks) / len(weighted_risks)
        else:
            org_risk = 0.0
        
        return min(10.0, org_risk)
    
    def _generate_vulnerability_summary(self, vulnerabilities: List[VulnerabilityFinding]) -> Dict[str, Any]:
        """Generate summary statistics for vulnerabilities."""
        if not vulnerabilities:
            return {
                'total_vulnerabilities': 0,
                'severity_distribution': {'critical': 0, 'high': 0, 'medium': 0, 'low': 0},
                'average_severity': 0.0,
                'average_confidence': 0.0
            }
        
        severity_distribution = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0}
        
        for vuln in vulnerabilities:
            if vuln.severity_score >= 9.0:
                severity_distribution['critical'] += 1
            elif vuln.severity_score >= 7.0:
                severity_distribution['high'] += 1
            elif vuln.severity_score >= 4.0:
                severity_distribution['medium'] += 1
            else:
                severity_distribution['low'] += 1
        
        return {
            'total_vulnerabilities': len(vulnerabilities),
            'severity_distribution': severity_distribution,
            'average_severity': sum(v.severity_score for v in vulnerabilities) / len(vulnerabilities),
            'average_confidence': sum(v.confidence_level for v in vulnerabilities) / len(vulnerabilities)
        }
    
    def _assess_compliance_gaps(
        self, 
        vulnerabilities: List[VulnerabilityFinding], 
        requirements: List[str]
    ) -> List[Dict[str, Any]]:
        """Assess gaps against compliance requirements."""
        gaps = []
        
        for requirement in requirements:
            framework = self.compliance_frameworks.get(requirement, {})
            if not framework:
                continue
            
            # Count vulnerabilities that violate this framework
            violations = []
            for vuln in vulnerabilities:
                if self._violates_compliance_requirement(vuln, requirement, framework):
                    violations.append({
                        'vulnerability_id': vuln.id,
                        'violation_type': self._get_violation_type(vuln, framework),
                        'severity': vuln.severity_score
                    })
            
            if violations:
                gaps.append({
                    'compliance_framework': requirement,
                    'violation_count': len(violations),
                    'violations': violations,
                    'risk_level': 'high' if len(violations) > 3 else 'medium' if len(violations) > 1 else 'low'
                })
        
        return gaps
    
    def _violates_compliance_requirement(
        self, 
        vuln: VulnerabilityFinding, 
        requirement: str, 
        framework: Dict[str, Any]
    ) -> bool:
        """Check if vulnerability violates compliance requirement."""
        # Simple heuristic based on vulnerability type and severity
        if requirement == "eu_ai_act":
            return vuln.vulnerability_type in [VulnerabilityType.BIAS_EXPLOITATION, 
                                             VulnerabilityType.INFORMATION_LEAKAGE] and vuln.severity_score > 6.0
        elif requirement == "gdpr":
            return vuln.vulnerability_type in [VulnerabilityType.INFORMATION_LEAKAGE,
                                             VulnerabilityType.MODEL_INVERSION,
                                             VulnerabilityType.MEMBERSHIP_INFERENCE] and vuln.severity_score > 5.0
        elif requirement == "pci_dss":
            return vuln.severity_score > 7.0  # Any high severity vulnerability
        elif requirement == "hipaa":
            return vuln.vulnerability_type in [VulnerabilityType.INFORMATION_LEAKAGE,
                                             VulnerabilityType.MODEL_INVERSION] and vuln.severity_score > 4.0
        
        return False
    
    def _get_violation_type(self, vuln: VulnerabilityFinding, framework: Dict[str, Any]) -> str:
        """Get the type of compliance violation."""
        if vuln.vulnerability_type == VulnerabilityType.INFORMATION_LEAKAGE:
            return "data_privacy_violation"
        elif vuln.vulnerability_type == VulnerabilityType.BIAS_EXPLOITATION:
            return "fairness_violation"
        elif vuln.vulnerability_type in [VulnerabilityType.ADVERSARIAL_ATTACKS, VulnerabilityType.DATA_POISONING]:
            return "security_control_failure"
        else:
            return "general_security_violation"
    
    def _compare_to_industry_benchmark(
        self, 
        overall_risk: float, 
        vulnerabilities: List[VulnerabilityFinding], 
        industry: str
    ) -> Dict[str, Any]:
        """Compare risk profile to industry benchmarks."""
        benchmark = self.industry_benchmarks.get(industry, {})
        if not benchmark:
            return {}
        
        critical_count = sum(1 for v in vulnerabilities if v.severity_score >= 9.0)
        high_count = sum(1 for v in vulnerabilities if 7.0 <= v.severity_score < 9.0)
        
        return {
            'industry': industry,
            'risk_score_comparison': {
                'current': overall_risk,
                'benchmark_max': benchmark.get('max_acceptable_risk', 5.0),
                'status': 'exceeds_benchmark' if overall_risk > benchmark.get('max_acceptable_risk', 5.0) else 'within_benchmark'
            },
            'vulnerability_comparison': {
                'critical_vulnerabilities': {
                    'current': critical_count,
                    'benchmark_max': benchmark.get('critical_vulnerabilities_threshold', 0),
                    'status': 'exceeds_benchmark' if critical_count > benchmark.get('critical_vulnerabilities_threshold', 0) else 'within_benchmark'
                },
                'high_vulnerabilities': {
                    'current': high_count,
                    'benchmark_max': benchmark.get('high_vulnerabilities_threshold', 2),
                    'status': 'exceeds_benchmark' if high_count > benchmark.get('high_vulnerabilities_threshold', 2) else 'within_benchmark'
                }
            }
        }
    
    def _analyze_risk_trends(self, vulnerabilities: List[VulnerabilityFinding]) -> Dict[str, Any]:
        """Analyze risk trends and patterns."""
        vuln_types_count = {}
        severity_distribution = {'critical': 0, 'high': 0, 'medium': 0, 'low': 0}
        
        for vuln in vulnerabilities:
            # Count by type
            vuln_type = vuln.vulnerability_type.value
            vuln_types_count[vuln_type] = vuln_types_count.get(vuln_type, 0) + 1
            
            # Count by severity
            if vuln.severity_score >= 9.0:
                severity_distribution['critical'] += 1
            elif vuln.severity_score >= 7.0:
                severity_distribution['high'] += 1
            elif vuln.severity_score >= 4.0:
                severity_distribution['medium'] += 1
            else:
                severity_distribution['low'] += 1
        
        return {
            'vulnerability_types_distribution': vuln_types_count,
            'severity_distribution': severity_distribution,
            'total_vulnerabilities': len(vulnerabilities),
            'average_severity': sum(v.severity_score for v in vulnerabilities) / len(vulnerabilities) if vulnerabilities else 0,
            'most_common_vulnerability_type': max(vuln_types_count.items(), key=lambda x: x[1])[0] if vuln_types_count else None
        }
    
    def _generate_prioritized_recommendations(
        self, 
        vulnerabilities: List[VulnerabilityFinding], 
        systems: List[SystemProfile]
    ) -> List[Dict[str, Any]]:
        """Generate prioritized remediation recommendations."""
        recommendations = []
        
        # Sort vulnerabilities by environmental score (severity * confidence)
        sorted_vulns = sorted(vulnerabilities, key=lambda v: v.environmental_score, reverse=True)
        
        for i, vuln in enumerate(sorted_vulns[:10]):  # Top 10 vulnerabilities
            recommendations.append({
                'priority_rank': i + 1,
                'vulnerability_id': vuln.id,
                'vulnerability_name': vuln.name,
                'affected_systems': vuln.affected_systems,
                'risk_score': vuln.environmental_score,
                'recommended_actions': vuln.mitigation_recommendations,
                'estimated_effort': self._estimate_remediation_effort(vuln),
                'business_impact_if_not_addressed': self._estimate_business_impact(vuln, systems)
            })
        
        return recommendations
    
    def _estimate_remediation_effort(self, vuln: VulnerabilityFinding) -> str:
        """Estimate effort required for remediation."""
        if vuln.vulnerability_type in [VulnerabilityType.PROMPT_INJECTION, VulnerabilityType.JAILBREAKING]:
            return "medium"  # Requires prompt engineering and input validation
        elif vuln.vulnerability_type in [VulnerabilityType.ADVERSARIAL_ATTACKS, VulnerabilityType.EVASION_ATTACKS]:
            return "high"  # Requires model retraining or architectural changes
        elif vuln.vulnerability_type in [VulnerabilityType.DATA_POISONING, VulnerabilityType.BACKDOOR_ATTACKS]:
            return "very_high"  # Requires data cleaning and model rebuilding
        else:
            return "low"  # Can be addressed with configuration changes
    
    def _estimate_business_impact(self, vuln: VulnerabilityFinding, systems: List[SystemProfile]) -> str:
        """Estimate business impact if vulnerability is not addressed."""
        affected_systems = [s for s in systems if s.name in vuln.affected_systems]
        
        if not affected_systems:
            return "low"
        
        max_criticality = max(s.criticality for s in affected_systems)
        
        if max_criticality == CriticalityLevel.CRITICAL and vuln.severity_score >= 8.0:
            return "severe"
        elif max_criticality == CriticalityLevel.HIGH and vuln.severity_score >= 7.0:
            return "high"
        elif vuln.severity_score >= 6.0:
            return "medium"
        else:
            return "low"
    
    def generate_executive_report(
        self,
        risk_scores: Dict[str, Any],
        business_impact_analysis: bool = True,
        mitigation_cost_benefit: bool = True,
        regulatory_compliance_gaps: bool = True,
        board_presentation_ready: bool = True
    ) -> str:
        """Generate executive-level risk assessment report."""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        report = f"""
# AI Security Risk Assessment Report
**Assessment Date:** {timestamp}
**Overall Risk Score:** {risk_scores['overall_risk_score']:.1f}/10.0

## Executive Summary

### Risk Overview
- **Total AI Systems Assessed:** {len(risk_scores['system_risk_scores'])}
- **Total Vulnerabilities Identified:** {len(risk_scores['vulnerabilities'])}
- **Critical Vulnerabilities:** {risk_scores['vulnerability_summary']['severity_distribution']['critical']}
- **High Severity Vulnerabilities:** {risk_scores['vulnerability_summary']['severity_distribution']['high']}

### Risk Level Classification
"""
        
        overall_risk = risk_scores['overall_risk_score']
        if overall_risk >= 8.0:
            risk_level = "🔴 CRITICAL"
            risk_description = "Immediate action required. Significant business risk."
        elif overall_risk >= 6.0:
            risk_level = "🟡 HIGH"
            risk_description = "Urgent attention needed. Moderate business risk."
        elif overall_risk >= 4.0:
            risk_level = "🟨 MEDIUM"
            risk_description = "Should be addressed in next planning cycle."
        else:
            risk_level = "🟢 LOW"
            risk_description = "Acceptable risk level. Monitor and maintain."
        
        report += f"**Current Risk Level:** {risk_level}\n**Risk Assessment:** {risk_description}\n\n"
        
        # System-level risk breakdown
        report += "## System Risk Breakdown\n"
        for system_name, system_risk in risk_scores['system_risk_scores'].items():
            report += f"- **{system_name}:** {system_risk['risk_score']:.1f}/10.0 "
            report += f"({system_risk['vulnerability_count']} vulnerabilities)\n"
        
        # Compliance gaps
        if regulatory_compliance_gaps and risk_scores.get('compliance_gaps'):
            report += "\n## Regulatory Compliance Gaps\n"
            for gap in risk_scores['compliance_gaps']:
                report += f"- **{gap['compliance_framework']}:** {gap['violation_count']} violations "
                report += f"({gap['risk_level']} risk)\n"
        
        # Top recommendations
        if risk_scores.get('prioritized_recommendations'):
            report += "\n## Top Priority Recommendations\n"
            for rec in risk_scores['prioritized_recommendations'][:5]:
                report += f"{rec['priority_rank']}. **{rec['vulnerability_name']}**\n"
                report += f"   - Risk Score: {rec['risk_score']:.1f}\n"
                report += f"   - Estimated Effort: {rec['estimated_effort']}\n"
                report += f"   - Business Impact: {rec['business_impact_if_not_addressed']}\n\n"
        
        # Industry benchmark comparison
        if risk_scores.get('industry_benchmark_comparison'):
            benchmark = risk_scores['industry_benchmark_comparison']
            report += f"\n## Industry Benchmark Comparison\n"
            report += f"**Industry:** {benchmark.get('industry', 'N/A')}\n"
            
            risk_comparison = benchmark.get('risk_score_comparison', {})
            if risk_comparison.get('status') == 'exceeds_benchmark':
                report += f"⚠️ **Risk Score:** {risk_comparison['current']:.1f} "
                report += f"(exceeds industry benchmark of {risk_comparison['benchmark_max']:.1f})\n"
            else:
                report += f"✅ **Risk Score:** Within industry benchmark\n"
        
        if board_presentation_ready:
            report += "\n## Key Metrics for Board Presentation\n"
            report += f"- **Security Posture:** {'Poor' if overall_risk >= 7 else 'Fair' if overall_risk >= 4 else 'Good'}\n"
            report += f"- **Immediate Action Required:** {'Yes' if overall_risk >= 6 else 'No'}\n"
            report += f"- **Regulatory Risk:** {'High' if len(risk_scores.get('compliance_gaps', [])) > 0 else 'Low'}\n"
        
        return report

def main():
    """Example usage of VulnerabilityScorer."""
    logging.basicConfig(level=logging.INFO)
    
    # Example system configuration
    assessment_config = {
        'ai_systems_inventory': [
            {
                'name': 'customer_service_ai',
                'type': 'chatbot',
                'criticality': 'high',
                'exposure': 'public',
                'data_sensitivity': 'confidential',
                'business_impact': {'financial': 0.7, 'operational': 0.8, 'reputational': 0.9}
            },
            {
                'name': 'fraud_detection_ai',
                'type': 'classifier',
                'criticality': 'critical',
                'exposure': 'internal',
                'data_sensitivity': 'restricted',
                'business_impact': {'financial': 0.9, 'operational': 0.6, 'reputational': 0.7}
            }
        ]
    }
    
    # Example vulnerability data
    attack_test_results = {
        'customer_service_ai': {
            'vulnerabilities': [
                {
                    'id': 'vuln_001',
                    'name': 'Prompt Injection Vulnerability',
                    'type': 'prompt_injection',
                    'exploitability': 8.0,
                    'impact': 6.0,
                    'confidence': 0.9,
                    'description': 'AI system can be manipulated through crafted prompts',
                    'evidence': {'proof_of_concept': True, 'test_results': True}
                }
            ]
        }
    }
    
    # Initialize scorer and run assessment
    scorer = VulnerabilityScorer()
    
    results = scorer.calculate_comprehensive_risk(
        assessment_config=assessment_config,
        attack_test_results=attack_test_results,
        industry_benchmarks='financial_services',
        regulatory_requirements=['gdpr', 'pci_dss']
    )
    
    # Generate executive report
    report = scorer.generate_executive_report(results)
    
    print(f"Assessment completed. Overall risk score: {results['overall_risk_score']:.2f}")
    print(f"Total vulnerabilities: {len(results['vulnerabilities'])}")
    print("\nExecutive Report:")
    print(report)

if __name__ == "__main__":
    main()